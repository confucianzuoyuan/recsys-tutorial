== 机器学习模型简介

主要内容

*监督学习*

* 回归模型
** 线性回归

* 分类模型
** 感知机
** K近邻（KNN）
** 决策树
** 逻辑斯蒂回归

*无监督学习*

* 聚类
** K均值聚类

=== 使用Anaconda

https://zhuanlan.zhihu.com/p/32925500[点击链接学习如何安装和使用Anaconda]

=== 线性回归

* 线性回归（linear regression）是一种线性模型，它假设输入变量x和单个输出变量y之间存在线性关系
* 具体来说，利用线性回归模型，可以从一组输入变量x的线性组合中，计算输出变量y

* 一元线性回归(只有一个特征stem:[x])

[stem]
++++
y=wx+b
++++

* 多元线性回归(有多个特征stem:[x_0,x_1,\dots,x_n])

[stem]
++++
f(x) = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
++++

image::xianxinghuigui.png[]

NOTE: 一元线性回归就是从上图中的一堆散点图，拟合出一条直线stem:[y=wx+b]。这里的拟合其实就是机器学习。散点为训练数据，我们要从训练数据中学习出一条直线。

这里的模型就是线性回归模型。那么机器学习的算法呢？

* 最小二乘法
* 梯度下降法

NOTE: 注意我们这里还没有讲监督学习三要素中的策略呢！

==== 损失函数

这里使用平方损失函数，并且没有正则化项。

[stem]
++++
L(w,b) = \sum_{i=1}^m (wx_i + b - y_i)^2
++++

==== 学习算法

求解stem:[w,b]使得平方损失函数取得最小值。

* 最小二乘法
* 梯度下降法

==== 最小二乘法

* 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method）。
* 它的主要思想就是选择未知参数，使得理论值与观测值之差的平方和达到最小。

image::zuixiaoercheng.png[]

* 我们假设输入属性（特征）的数目只有一个：就是stem:[f(x)=wx+b]中的stem:[x]。我们的训练数据是stem:[(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)]。要从这些训练数据中学习一个模型出来，使得：stem:[f(x_i)=wx_i+b, f(x_i) \approx y_i]。

* 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。

[stem]
++++
\begin{align}
(w^*,b^*) &= \arg \min_{(w,b)}\sum_{i=1}^m (f(x_i)-y_i)^2 \\
          &= \arg \min_{(w,b)}\sum_{i=1}^m (wx_i+b-y_i)^2
\end{align}
++++

NOTE: 以上就是监督学习三要素中的策略！

===== 最小二乘法求解过程

* 求解stem:[w]和stem:[b]，使得stem:[E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2]最小化的过程，称为线性回归模型的“最小二乘估计”。

* 将stem:[E_{(w,b)}]分别对stem:[w]和stem:[b]进行求导，可以得到：

[stem]
++++
\begin{align}
\frac{\partial E_{(w,b)}}{\partial w} 
&= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&= 2(wx_1+b-y_1)x_1+ \dots +2(wx_m+b-y_m)x_m \\
&= 2(wx_1^2-(y_1-b)x_1) + \dots + 2(wx_m^2-(y_m-b)x_m) \\
&= 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\

\frac{\partial E_{(w,b)}}{b}
&= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&= 2(wx_1+b-y_1) + \dots + 2(wx_m+b-y_m) \\
&= 2(mb-\sum_{i=1}^m (y_i-wx_i))
\end{align}
++++

* 令偏导数都为0，可以得到：

[stem]
++++
2(mb-\sum_{i=1}^m (y_i-wx_i)) = 0 \\
b = \frac{1}{m}\sum_{i=1}^m (y_i-wx_i)
++++

将stem:[b]代入到第一个式子中，可以解得stem:[w]的值，然后再将stem:[w]的值代入到b的表达式，即可解得stem:[b]。

[stem]
++++
w = \frac{\sum_{i=1}^m y_i(x_i-\bar{x})}{\sum_{i=1}^m x_i^2-\frac{1}{m}(\sum_{i=1}^m x_i)^2} \\
b = \frac{1}{m}\sum_{i=1}^m (y_i-wx_i)
++++

其中：stem:[\bar{x}=\frac{1}{m}\sum_{i=1}^m x_i]

==== 梯度下降法求解过程

求解stem:[w]和stem:[b]，使得stem:[E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2]达到极小值，也可以使用梯度下降法。

梯度为：

[stem]
++++
\nabla E = 
[\frac{\partial}{\partial w} E_{(w,b)}, \frac{\partial}{\partial b} E_{(w,b)}] \\
\frac{\partial E_{(w,b)}}{\partial w} = 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\
\frac{\partial E_{(w,b)}}{b} = 2(mb-\sum_{i=1}^m (y_i-wx_i))
++++

NOTE: 这里不是令偏导数为零了，而是准备梯度下降。

[stem]
++++
w_{next} := w_{current} - \alpha \frac{\partial E}{\partial w} \\
b_{next} := b_{current} - \alpha \frac{\partial E}{\partial b}
++++

NOTE: 以上，stem:[w,b]的初始值，stem:[\alpha]的值，以及迭代的次数，都是超参数。因为不是机器学习学习出来的参数。

* stem:[\alpha]在梯度下降算法中被称作为学习率或者步长
* 这意味着我们可以通过stem:[\alpha]来控制每一步走的距离，以保证不要走太快，错过了最低点；同时也要保证收敛速度不要太慢
* 所以stem:[\alpha]的选择在梯度下降法中往往是很重要的，不能太大也不能太小

image::xuexilv.png[]

==== 梯度下降法和最小二乘法

* 相同点
** 本质和目标相同：两种方法都是经典的学习算法，在给定已知数据的前提下利用求导算出一个模型（函数），使得损失函数最小，然后对给定的新数据进行估算预测

* 不同点
** 损失函数：梯度下降可以选取其它损失函数，而最小二乘一定是平方损失函数
** 实现方法：最小二乘法是直接求导找出全局最小；而梯度下降是一种迭代法
** 效果：最小二乘找到的一定是全局最小，但计算繁琐，且复杂情况下未必有解；梯度下降迭代计算简单，但找到的一般是局部最小，只有在目标函数是凸函数时才是全局最小；到最小点附近时收敛速度会变慢，且对初始点的选择极为敏感

=== 感知机（Perceptron）

==== 感知机的定义

感知机是机器学习应用中分类的最简单的一种算法。如下图所示：感知机划分超平面

image::ganzhiji1.png[]

感知机是二分类的线性模型，输入是实例的特征向量，输出是实例的类别；假设训练的数据集是线性可分的，感知机的目标就是求一个能够将训练集的正负样本完全正确分离开的超平面(也就是上图中所示的那些将蓝、黄数据点完全分离开的直线)。但是如果这些数据是非线性可分的，这个超平面是无法获取的。上图的坐标轴，横坐标为stem:[X_1]，纵坐标为stem:[X_2]。图中的每一个点都由stem:[(X_1,X_2)]所决定。举个实例：有一批零件，判断零件是否合格有两个重要点，长度和重量。stem:[X_1]表示长度，stem:[X_2]表示重量，上图的两条黑线表示零件的长度均值和重量均值。只有当长度和重量都满足一定条件，该零件才为合格品。都不满足一定条件，视为不可修复的劣质品，直接丢弃。那么机器学习如何学习到这个规则呢？我们在代码实现的时候，拿到手的是所有样本的信息stem:[(X_1,X_2)]和标签(0或1)，标签里面0表示不合格品，1表示合格品。简单的说就是图片上黄色和蓝色的点。根据我们手上的这些点，我们需要找到一条直线将上面的点完美的分开。这样的直线我们可以找到很多条，那么哪一条才是最好的呢？实际上，感知机只是一个二分类的问题，无法找到一条最佳的直线，只需要能把所有的点都分开就好。我们设定损失函数为所有分错的点和直线的距离求和，然后训练，使这段求和的数值最小(最优的情况是0，因为0代表完全分开了)，那么这条直线就满足我们的条件，就是我们所找的。

==== 感知机的数学原理

首先，点stem:[P(x_0,y_0)]到直线stem:[Ax+By+C=0]的距离为：

[stem]
++++
d=\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}
++++

类似的：设超平面公式为：stem:[h=wx+b]，其中stem:[w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)]。其中样本点stem:[x']到超平面的距离为：

[stem]
++++
d=\frac{wx'+b}{\parallel w \parallel}
++++

那么这个超平面为什么设置为stem:[wx+b]呢？它和我们常见的stem:[ax+b]有什么区别呢？

本质没啥区别，stem:[ax+b]是二维中的，stem:[wx+b]是高维中的。就看你的理解啦，简单的来说，stem:[wx+b]是一个stem:[n]维空间中的超平面stem:[S]，其中stem:[w]是超平面的法向量，stem:[b]是超平面的截距，这个超平面将特征空间划分成两部分，位于两部分的点分别被分为正负两类，所以，超平面S称为分离超平面。其中stem:[w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)]。

细节：

stem:[w]是超平面的法向量：对于一个平面来说stem:[w]就是这么定义的。数学上就这么定义的。

stem:[b]是超平面的截距：可以按照二维中的stem:[ax+b]理解。

特征空间：也就是整个stem:[n]维空间，样本的每个属性都叫一个特征，特征空间的意思就是在这个空间中可以找到样本所有的属性组合。

==== 感知机的模型

image::ganzhiji2.png[]

感知机的模型：输入空间—>输出空间：

[stem]
++++
f(x)=sign(wx+b), 其中, sign(x)=
\begin{cases}
  -1 & x < 0 \\
  1 & x \ge 0
\end{cases}
++++

sign函数很简单，当x大于等于0，sign输出1，否则输出-1。那么往前想一下，stem:[wx+b]如果大于等于0，stem:[f(x)]就等于1，反之stem:[f(x)]等于-1。

==== 感知机的损失函数

我们定义样本stem:[(x_i,y_i)]，如果上面的距离stem:[d > 0]，则stem:[y_i=1]；如果stem:[d < 0],则stem:[y_i=-1]，这样取stem:[y]有一个好处，就是方便定义损失函数。优化的目标：期望使误分类的所有样本，到超平面的距离之和最小。

所以定义损失函数为：

[stem]
++++
L(w,b)=-\frac{1}{\parallel w \parallel} \sum_{x_i \in M}y_i(wx_i+b)
++++

其中M集合就是误分类点的集合。

不考虑前面的系数，感知机模型的损失函数为：

[stem]
++++
L(w,b)=-\sum_{x_i \in M}y_i(wx_i+b)
++++

==== 感知机学习算法

感知机学习算法是对于上述损失函数进行极小化，求得stem:[w]和stem:[b]。这里使用随机梯度下降法(SGD)，因为误分类的M集合里面的样本才能参加损失函数的优化。

目标函数如下：

[stem]
++++
L(w,b)=\arg \min_{w,b}(-\sum_{x_i \in M}y_i(wx_i+b))
++++

*算法步骤*

====
输入：训练数据集stem:[T=(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N),y_i \in \{-1,+1\},学习率 \eta (0 < \eta < 1)]

输出：stem:[w,b]；感知机模型stem:[f(x)=sign(wx+b)]

. 赋初值stem:[w_0,b_0]
. 选取数据点stem:[(x_i,y_i)]
. 判断该数据点是否为当前模型的误分类点，即判断若stem:[y_i(wx_i+b) \le 0]则更新：

[stem]
++++
w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i
++++

[start=4]
. 转到2，直到训练集中没有误分类点。
====

=== 逻辑回归（Logistic Regression）

==== 直观理解

NOTE: 逻辑回归虽然叫回归，但其实是分类算法。

我们还记不记得感知机一节中给出的这张图？还记得感知机是怎么工作的吗？

image::luojihuigui.png[]

感知机生成一个超平面（在二维中表现为一条直线），这个超平面可以将图中的两类点区分开。像下面这张图，图中的所有直线都可以作为感知机的超平面（因为它们都能把训练集中的点完美地分开）。

image::luojihuigui2.png[]

但是感知机存在一个很重要的问题，我们只用stem:[sign(wx+b)]输出的stem:[+1]和stem:[-1]来判断点的类别是不是太简单了？是不是有点生硬的感觉。

这么简单的判别方式真的会很有效吗？

当然了，虽然我们已经程序测试过正确率很高，但总是让人有点担心是否在很多情况下都能很好地工作。事实上我们从小到大一直会听到一些升学考试差一分两分的例子，那么差一分和高一分的学生真的就是天壤之别吗？感知机也是如此：在超平面左侧距离原点0.001的点和右侧距离原点0.001的点输出就是+1和-1这天壤之别的差距真的合适吗？

image::luojihuigui3.png[]

此外我们也知道机器学习中通常会对目标函数进行微分（求导）进而梯度下降，但我们看上面这张图。很明显stem:[x=0]是跳跃间断点，因此stem:[sign]是一条不光滑的函数，没有办法进行微分。咦？那我记得感知器用了梯度下降，它是怎么去梯度下降的？

我们回忆一下感知器的梯度下降方式，确实用了梯度下降，但是发现没有，它把sign的壳子去掉了，对sign内部进行梯度下降，相对于其他能直接微分的算法来说，感知器的这种方式确实有点不太好。

感知机算法：

====
输入：训练数据集stem:[T=(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N),y_i \in \{-1,+1\},学习率 \eta (0 < \eta < 1)]

输出：stem:[w,b]；感知机模型stem:[f(x)=sign(wx+b)]

. 赋初值stem:[w_0,b_0]
. 选取数据点stem:[(x_i,y_i)]
. 判断该数据点是否为当前模型的误分类点，即判断若stem:[y_i(wx_i+b) \le 0]则更新：

[stem]
++++
w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i
++++

[start=4]
. 转到2，直到训练集中没有误分类点。
====

因此既然我们要对感知器进行优化，那么上文的这两个主要的缺陷咱们得想想法子看看能不能弥补。我们首先试试解决第一个问题：

1. 怎么解决极小距离差别带来的+1和-1的天壤之别？

在逻辑斯蒂回归中大致思想与感知器相同，但在输出stem:[+1]与stem:[-1]之间存在一些差别。我们知道stem:[P(Y|X)]，它表示在给定stem:[X]条件下，stem:[Y]发生的概率。逻辑回归也使用了同样的概念，它使用stem:[p(Y=-1|X)]和stem:[p(Y=1|X)]来表示该样本分别为stem:[-1]或stem:[1]的概率（实际上逻辑回归并非强制要求标签必须为1或-1，可以用任意标签来表示）。这样当再出现样本stem:[X_1]距离为stem:[-0.001]时，可能stem:[P(Y=1|X_1)=0.49,P(Y=0|X_1)=0.51]，那么我们觉得stem:[X_1]为0的概率更大一点，但我们同时也清楚程序可能并不太确定stem:[X_1]一定为stem:[0]。

使用概率作为输出结果使得样本在距离很小的差别下不再强制地输出stem:[+1]和stem:[-1]这两种天壤之别的结果，而是通过概率的方式告诉你结果可能是多少，同时也告诉你预测的不确信程度。这样子看起来让人比较安心一点不是吗？

2. 怎么让最终的预测式子可微呢？

虽然无法微分并不会阻碍感知器的正常工作（事实上只是避开了sign），但对于很多场合都需要微分的机器学习来说，能找到一个可以微分的最终式子是很重要的。为了解决第一个问题我们提出了一种概率输出模型，那么感知器的stem:[sign]也需要被随之替换为一种能输出概率大小的函数。具体函数在下文中详细讲解，其中值得高兴的是，我们找到的概率输出式子是平滑的，可微的，所以第二个问题也就迎刃而解了。

==== 逻辑回归模型

与感知器一样，我们先不管内部怎么工作，最好能够得到一个函数stem:[f(x)]，我们将样本stem:[x]放进去以后，它告诉我们属于stem:[1]的概率是多少。比如说stem:[f(y=1|x)=0.2,f(y=0|x)=0.8]，我们就知道该样本的标签大概率是stem:[0]。总结一下也就是分别比较两种类别的概率大小，概率大的那一方则作为预测类别进行输出。stem:[f]函数的定义如下所示。

二项逻辑回归模型是如下的条件概率分布：

[stem]
++++
P(Y=1|x)=\frac{\exp{(wx+b)}}{1+\exp{(wx+b)}} \\
P(Y=0|x)=\frac{1}{1+\exp{(wx+b)}}
++++

这里，stem:[x \in R^n]是输入，stem:[Y \in \{ 0,1 \}]是输出，stem:[w \in R^n]和stem:[b \in R]是参数，stem:[w]称为权值向量(weight)，stem:[b]称为偏置(bias)，stem:[w \cdot x]为stem:[w]和stem:[x]的内积。

下图就是stem:[f(w \cdot x+b)]的图像，我们假设一下点stem:[X]在超平面右边，那么距离应当为正，如果距离无穷大时，stem:[\exp{(w \cdot x + b)}]无穷大，stem:[P(Y=1|x)=1]，也就是概率为stem:[1]，表示极其确信。如果stem:[w \cdot x+b]是一个很接近stem:[0]的数，stem:[\exp(w \cdot x + b)]接近stem:[1]，stem:[P(Y=1|x)=0.5]，表示两边都有可能，不太确定。我们对于每一个样本点分别计算属于两类的概率，概率较大的一方作为预测的输出。

image::luojihuigui4.png[]

NOTE: 其实这里的函数stem:[\phi (z)=\frac{1}{1+e^{-z}}]，有一个很有名的名字叫做Sigmoid Function，或者叫压缩函数，在神经网络里面经常用到。

==== 损失函数

好了，所要用的几个函数我们都有了，接下来要做的就是根据给定的训练集，把参数stem:[w]给求出来了。要找参数stem:[w]，首先就是得把损失函数（cost function）给定义出来，也就是目标函数。

我们第一个想到的自然是模仿线性回归的做法，利用误差平方和来当代价函数。

[stem]
++++
L(w,b)=\frac{1}{2} \sum_{i=1}^N (\phi (w \cdot x_i + b)-y_i)^2
++++

其中stem:[(x_i,y_i)]为样本点。stem:[y_i]为真实值，stem:[\phi (w \cdot x_i + b)]为预测值。

我们发现，逻辑回归的平方损失函数是一个非凸函数，这就意味着损失函数有着许多的局部极小值，这不利于我们求解最小值。

image::luojihuigui5.png[]

所以我们这里使用**对数损失函数**。

* 对数损失函数

[stem]
++++
L(Y,P(Y \vert X))=-logP(Y \vert X)
++++

也就是说

[stem]
++++
L(w,b)=
\begin{cases}
  -\log \phi (wx+b) & if y = 1 \\
  -\log (1-\phi (wx+b)) & if y = 0
\end{cases}
++++

image::luojihuigui6.png[]

也就是说

当样本值stem:[y=1]，而预测值stem:[\phi (wx+b) = \frac{\exp{(wx+b)}}{1+\exp{(wx+b)}}]接近于0时，也就是说完全预测错误，那么惩罚将是很大的，是无穷大。stem:[y=0]时同理。

我们可以得出

[stem]
++++
L(w,b)=
\begin{cases}
  -\log \phi (wx+b) & if y = 1 \\
  -\log (1-\phi (wx+b)) & if y = 0
\end{cases} \\
\Downarrow \\
L(w,b) = -y \log \phi (wx+b) - (1 - y) \log (1 - \phi (wx+b)) \\
\Downarrow \\
L(w,b) = \frac{1}{m} \sum_{i=1}^m (-y_i \log \phi (wx_i+b) - (1 - y_i) \log (1 - \phi (wx_i+b)))
++++

==== 梯度下降

NOTE: Sigmoid Function有一个很好的性质：

[stem]
++++
\phi ' (z)=\phi (z)(1-\phi (z))
++++

求偏导数的过程

[stem]
++++
\frac{\partial L(w,b)}{\partial w}
=
\sum_{i=1}^m (y_ix_i-\frac{x_i \exp (wx_i)}{1+ \exp (wx_i)})
++++

=== KNN

* 最简单最初级的分类器，就是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类
* K近邻（k-nearest neighbour, KNN）是一种基本分类方法，通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数
* KNN算法中，所选择的邻居都是已经正确分类的对象

==== KNN例子

image::knn.png[]

* 绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？
* 如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类

* KNN算法的结果很大程度取决于K的选择

==== KNN距离计算

KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：

* 欧式距离

[stem]
++++
d(x,y) = \sqrt {\sum_{k=1}^n (x_k - y_k)^2}
++++

* 曼哈顿距离

[stem]
++++
d(x,y) = \sqrt {\sum_{k=1}^n | x_k - y_k |}
++++

==== KNN算法

====

在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

. 计算测试数据与各个训练数据之间的距离；
. 按照距离的递增关系进行排序；
. 选取距离最小的K个点；
. 确定前K个点所在类别的出现频率；
. 返回前K个点中出现频率最高的类别作为测试数据的预测分类。

====

=== 决策树

